# Bigset

## Why?

Set CRDTs stored in riak are just `riak_dt_orswot`s, binary encoded,
and stuffed in a `riak_object`. As a result they have a size limit of
about 1MB (as do all `riak_object`s.)

Another side effect of the design is that inserts/removes of an
element to a set takes O(n) time. Using the correct backing data
structure one would intuitively expect inserts to be O(1), and they
are, _if_ you have a `riak_dt_orswot` in memory. But riak is a
database that persists data to disk and riak is a
distributed/replciated database that replicates data over a network.

### Sets in Riak 2+

In riak 2+ the CRDT set is stored as a binary, inside a
`riak_object`. We took this decision so that a CRDT could be
replicated like a normal `riak_object`, AAE'd like a normal
`riak_object`, read repaired like a normal `riak_object`, and MDC'd
like a normal `riak_object`. In short, we wanted to do as little as
possible in terms of "teaching Riak" about CRDTs.


CRDTs in Riak 2+ are plumbed into riak in just a few places: an API
for the client libraries, an option that is passed through
`riak_kv_put_fsm`, a couple of lines in `riak_kv_vnode`, and a couple
of lines in `riak_object`'s `merge` logic. The bulk of the non-API
code is in `riak_kv_crdt`.

Throughout most of Riak a CRDT is just an opaque binary in the value
portion of a `riak_object`. This means a CRDT like a Set has two
version vectors! The one in `riak_object`, and the one in the Set
itself.

![Sets in Riak: Matryoshka!](sets_in_riak.png "Sets in Riak: Matryoshka")

#### Writes

When adding to a set in riak today the client sends an *Operation* to
Riak, something like:

    add 'X' to Set 'Y'

At the API boundary this is parsed, validated, and stuck into a record
(`crdt_op`) that contains fields for the module (`riak_dt_orswot`),
the op (`{add, 'X'}`) and a context (not used for adds, see below on
"Causal Consistency" as I believe this is a bug.)

A call to `riak_kv_crdt:new/3` returns an object that has an
empty/bottom value `riak_dt_orswot` for merging with whatever maybe
stored in Riak, and the operation is then applied inside the
vnode. The FSM does what it does for a regular PUT, and passes on the
`#crdt_op{}` record from the `Options` list to the coordinating vnode.

In the coordinating vnode, Riak does the following:

* Read the local value
* Treat incoming object as a sibling (it has no vclock, so it is a
  sibling by riak's vnode vclock rules)
* Deserialise the CRDT on disk
* Merge the incoming bottom with the present CRDT (if there is one)
* Apply the operation to the result
* Serialise the new value in a `riak_object`
* Write object to disk
* Return object to the FSM for replicating

This may all seem a little wasteful. In actual fact it is even worse,
as the merge causes the CRDT to be de-serialised, and re-serialised,
and then it is de-serialised again for the operation application, and
finally serialised for storage/replication. This was a bug/mistake and
can be addressed. However O(4n) is still O(n).

The Put FSM will then send the new value to N-1 replicas. At each
replica the following occurs:

* Read the local value
* If the incoming object's version vector descends the local:
    * Write the new value to disk
* If the local descends the incoming
    * discard incoming value
* If they are concurrent
    * run `riak_object:merge/2` which in turn calls `riak_kv_crdt:merge/2` which will
        * De-serialise the local value
        * deserialise the incoming value
        * run `riak_dt_orswot:merge/2` on the two values
    * finally serialise the result, and store on disk

There may be many things we can do to optimise this: have a single
format for in-memory and on disk/wire that does not require
serialisation seems the first step, but there is still the cost of
reading a potentially large object off disk simply to add a single
element. We replicate that whole object (see Deltas below if you're
screaming "Deltas! Deltas!" right now.) However, even if we optimise
the current implementation, there is still that `riak_object` size
limit of ~1MB.

![Sets in Riak: Inserts, not O(1)](dt-add.png "Sets in Riak: Inserts, not O(1)")

Above is a plot generated by running a single riak node, a single
client, in a tight loop, adding ten thousand random words from my
mac's dictionary file to a single CRDT set.

Below is a basho_bench run ading one hundred thousand random keys to
one thousand sets, with 50 concurrent clients. Run on a cluster of 4
machines in the Basho Boston colo.

![Sets in Riak: "Big Data?"](riak-100kelements-1ksets-50workers.png "Sets in Riak: Big Data?")

Although the plot below might show the trend more clearly, thanks to
the shorter run time and less extreme outliers

![Sets in Riak: Shorter run](short-run-dt-writes.png "Sets in Riak:Shorter run")


#### Reads

Reading a CRDT Set is just like reading a regular
`riak_object`. Though if the `riak_object` version vectors indicate
divegence, `riak_object:reconcile/1` is called which causes each
sibling to be de-serialised, merged, and the final result to be
serialised again. A bit of a waste as the API boundary simply takes
care of deserialising the result, and calling
`riak_dt_orswot:value/1`. There is some protocol buffers/json
encoding, too. I can't imagine a way that reading a set of size `n`
was not an O(n) operation, can you?


![Sets in Riak: "Reading ~40x100k per second"](20workes-1ksets-pareto-read-10mins-dt.png "Sets in Riak: Reading ~40x100k per second")

The above plot shows read operations after the previous write
benchmark was run. 20 workers, 10 minutes, pareto distribution over
the one thousand sets, reading the full set. I don't think that is too
bad, though it could certainly be improved.

#### Queries

There are none. If you want to ask questions of your set (size, is 'X'
a member, 100 lowest members etc) you have to read it, and query it in
your application. This seems wasteful if you just want the first 100
members, or to know if 'X' is in the Set, and gets more wasteful the
larger the Set is.


### What About Deltas?

In
[Efficient State-based CRDTs by Delta-Mutation](http://arxiv.org/abs/1410.2803)
Almeida et al describe a technique for avoiding the cost of full state
replication. An operation on a CRDT generates a Delta, that can be
understood as a fragment of a CRDT. This delta can be replicated and
merged, and expresses just the change of the original operation. The
delta merge has the same properties of a full state merge: Idempotent,
commutative, associative. This means it can be delivered over
unreliable networks (AKA networks.)

This seems ideal for at least part of the issues outlined
above. However merely replacing the `riak_dt_orswot` in riak with a
`riak_dt_delta_orswot` is not enough. In fact, we tried, and it was
worse.

#### Accidental Optimisation

Recall the steps at the replica above for Set writes:

* Read the local value
* If the incoming object's version vector descends the local:
    * Write the new value to disk

With a delta this can never happen. That means, for a replicated delta
operation the action at the replica can only be:

* run `riak_object:merge/2` which in turn calls `riak_kv_crdt:merge/2` which will
    * De-serialise the local value
    * deserialise the incoming value
    * run `riak_dt_orswot:merge/2` on the two values
* finally serialise the result, and store on disk

Even when there is no concurrency the price must be paid of
deserialisation, merge, and reserialise. The only time an incoming
delta can ever replace what is on disk at a replica is when it is the
first received update. In the best case, the savings are only in the
size of the data sent over the network. The full read and update at
the coordinator is as above, as is the read and merge at the
replica. Sadly the plots are misplaced, but our experiments with this
showed deltas to perform mostly worse than full state replication in
riak.

### Summary

In summary, the answer to "why bigsets?" is that a set per-object is
inefficient and restrictive.

## What is Bigset?

Bigset is the temporary name for a prototype/proof of concept
idea. The aim is to engineer a system that takes advantage of the
delta-CRDT work cited above. The fundamental difference is that rather
than a set per-object, instead the Set is decomposed into multiple
keys. At least one key per element, and an extra key for metadata.

### Design Overview

So far bigset is not in riak, but is a riak_core project. You can find
it in [here](https://github.com/basho-bin/bigsets "Bigsets -
basho-bin"). I won't cover the riak-core-ness of the design, I'll assume
you know about rings, replicas and vnodes.

What follows is how the prototype works today, and what I imagine
would be the next steps, but I've been wrong before.

#### The backend

Bigsets requires a sorted backend, it uses leveldb, maybe other
backends are also suitable.

Logically at least, the biggest change is the backend storage of the
set. In riak one key maps to one value, in bigsets, we split a set
over multiple keys. See [Key Scheme](#key-scheme) for more details
about the logical on disk format. We trust the sorted property of
leveldb to ensure that each set is grouped together contiguously, and
the logical clock is the first key in each set.

![Sets in Bigsets: Decomposed](bigset-backend.png "Sets in Bigsets: Decomposed")

#### Hashing

In Riak a Bucket and Key pair are hashed to decide the preflist and
nodes that will store the data, in bigsets only the Set name is
hashed. This means that all the elements of a particular Set 'S' share
the same preference list, and are stored thefore in the same
locations. Does this mean we can model buckets as sets, and
riak_objects as elements and get something like the global logical
clocks work? Maybe.

### Write operations - Insert

As with Riak sets, the client sends an operation to the server, saying
`"add X to set Y"`. An FSM hashes the set name, and sends the
operation to a vnode




##### <a id="key-scheme"></a>Key Scheme



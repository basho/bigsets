# Bigset

## 1. Why?

Set CRDTs stored in riak are just `riak_dt_orswot`s, binary encoded,
and stuffed in a `riak_object`. As a result they have a size limit of
1MB (as do all `riak_object`s.)

Another side effect of the design is that inserts/removes of an
element to a set takes O(n) time. Using the correct backing data
structure one would intuitively expect inserts to be O(1), and they
are, _if_ you have a `riak_dt_orswot` in memory. But riak is a
database that persists data to disk and riak is a
distributed/replicated database that replicates data over a network.

### 2. Sets in Riak 2+

In riak 2+ the CRDT set is stored as a binary, inside a
`riak_object`. We took this decision so that a CRDT could be
replicated like a normal `riak_object`, AAE'd like a normal
`riak_object`, read repaired like a normal `riak_object`, and MDC'd
like a normal `riak_object`. In short, we wanted to do as little as
possible in terms of "teaching Riak" about CRDTs.


CRDTs in Riak 2+ are plumbed into riak in just a few places: an API
for the client libraries, an option that is passed through
`riak_kv_put_fsm`, a couple of lines in `riak_kv_vnode`, and a couple
of lines in `riak_object`'s `merge` logic. The bulk of the non-API
code is in `riak_kv_crdt`.

Throughout most of Riak a CRDT is just an opaque binary in the value
portion of a `riak_object`. This means a CRDT like a Set has two
version vectors! The one in `riak_object`, and the one in the Set
itself.

![Sets in Riak: Matryoshka!](sets_in_riak.png "Sets in Riak: Matryoshka")

#### 2.1. Writes

When adding to a set in riak today the client sends an *Operation* to
Riak, something like:

    add 'X' to Set 'Y'

At the API boundary this is parsed, validated, and stuck into a record
(`crdt_op`) that contains fields for the module (`riak_dt_orswot`),
the op (`{add, 'X'}`) and a context (not used for adds, see below on
[Consistency](#consistency) as I believe this is a bug.)

A call to `riak_kv_crdt:new/3` returns an object that has an
empty/bottom value `riak_dt_orswot` for merging with whatever maybe
stored in Riak, and the operation is then applied inside the
vnode. The FSM does what it does for a regular PUT, and passes on the
`#crdt_op{}` record from the `Options` list to the coordinating vnode.

In the coordinating vnode, Riak does the following:

* Read the local value
* Treat incoming object as a sibling (it has no vclock, so it is a
  sibling by riak's vnode vclock rules)
* Deserialise the CRDT on disk
* Merge the incoming bottom with the present CRDT (if there is one)
* Apply the operation to the result
* Serialize the new value in a `riak_object`
* Write object to disk
* Return object to the FSM for replicating

This may all seem a little wasteful. In actual fact it is even worse,
as the merge causes the CRDT to be de-serialized, and re-serialized,
and then it is de-serialized again for the operation application, and
finally serialized for storage/replication. This was a bug/mistake and
can be addressed. However O(4n) is still O(n).

The Put FSM will then send the new value to N-1 replicas. At each
replica the following occurs:

* Read the local value
* If the incoming object's version vector descends the local:
    * Write the new value to disk
* If the local descends the incoming
    * discard incoming value
* If they are concurrent
    * run `riak_object:merge/2` which in turn calls `riak_kv_crdt:merge/2` which will
        * De-serialize the local value
        * Deserialise the incoming value
        * run `riak_dt_orswot:merge/2` on the two values
    * finally serialize the result, and store on disk

There may be many things we can do to optimise this: have a single
format for in-memory and on disk/wire that does not require
serialisation seems the first step, but there is still the cost of
reading a potentially large object off disk simply to add a single
element. We replicate that whole object (see [Deltas](#deltas) below
if you're screaming "Deltas! Deltas!" right now.) However, even if we
optimise the current implementation, there is still that `riak_object`
size limit of ~1MB.

![Sets in Riak: Inserts, not O(1)](dt-add.png "Sets in Riak: Inserts, not O(1)")

Above is a plot generated by running a single riak node, a single
client, in a tight loop, adding ten thousand random words from my
mac's dictionary file to a single CRDT set.

Below is a basho_bench run adding one hundred thousand random elements
to one thousand sets, with 50 concurrent clients. Run on a cluster of
4 machines in the Basho Boston colo.

![Sets in Riak: "Big Data?"](riak-100kelements-1ksets-50workers.png "Sets in Riak: Big Data?")

Although the plot below might show the trend more clearly, thanks to
the shorter run time and less extreme outliers

![Sets in Riak: Shorter run](short-run-dt-writes.png "Sets in Riak:Shorter run")


#### 2.2. Reads

Reading a CRDT Set is just like reading a regular
`riak_object`. Though if the `riak_object` version vectors indicate
divergence, `riak_object:reconcile/1` is called which causes each
sibling to be de-serialized, merged, and the final result to be
serialized again. A bit of a waste as the API boundary simply takes
care of de-serialising the result, and calling
`riak_dt_orswot:value/1`. There is some protocol buffers/json
encoding, too. I can't imagine a way that reading a set of size `n`
was not an O(n) operation, can you?


![Sets in Riak: "Reading ~40x100k per second"](20workes-1ksets-pareto-read-10mins-dt.png "Sets in Riak: Reading ~40x100k per second")

The above plot shows read operations after the previous write
benchmark was run. 20 workers, 10 minutes, pareto distribution over
the one thousand sets, reading the full set. I don't think that is too
bad considering each set has 100k elements, though it could certainly
be improved.

#### 2.3. Queries

There are none. If you want to ask questions of your set (cardinality,
is 'X' a member, 100 lowest members etc) you have to read it, and
query it in your application. This seems wasteful if you just want the
first 100 members, or to know if 'X' is in the Set, and gets more
wasteful the larger the Set is.


### <a id="deltas"></a>3. What About Deltas?

In
[Efficient State-based CRDTs by Delta-Mutation](http://arxiv.org/abs/1410.2803)
Almeida et al describe a technique for avoiding the cost of full state
replication. An operation on a CRDT generates a Delta, that can be
understood as a fragment of a CRDT. This delta can be replicated and
merged, and expresses just the change of the original operation. The
delta merge has the same properties of a full state merge: Idempotent,
commutative, associative. This means it can be delivered over
unreliable networks (AKA networks.)

This seems ideal for at least part of the issues outlined
above. However merely replacing the `riak_dt_orswot` in riak with a
`riak_dt_delta_orswot` is not enough. In fact, we tried, and it was
worse.

#### 3.1. Accidental Optimisation

Recall the steps at the replica above for Set writes:

* Read the local value
* If the incoming object's version vector descends the local:
    * Write the new value to disk

With a delta this can never happen. That means, for a replicated delta
operation the action at the replica must always be:

* run `riak_object:merge/2` which in turn calls `riak_kv_crdt:merge/2` which will
    * De-serialize the local value
    * de-serialise the incoming value
    * run `riak_dt_orswot:merge/2` on the two values
* finally serialize the result, and store on disk

Even when there is no concurrency the price must be paid of
de-serialisation, merge, and re-serialise. The only time an incoming
delta can ever replace what is on disk at a replica is when it is the
first received update. In the best case, the savings are only in the
size of the data sent over the network. The full read and update at
the coordinator is as above, as is the read and merge at the
replica. Sadly the plots are misplaced, but our experiments with this
showed deltas to perform mostly worse than full state replication in
riak.

### 4. Summary

In summary, the answer to "why bigsets?" is that a set per-object is
inefficient and restrictive.

## 5. What is Bigset?

Bigset is the temporary name for a prototype/proof of concept
idea. The aim is to engineer a system that takes advantage of the
delta-CRDT work cited above. The fundamental difference is that rather
than a set per-object, instead the Set is decomposed into multiple
keys. At least one key per element, and extra keys for metadata.

## 6. Design Overview

So far bigset is not in riak, but is a riak_core project. You can find
it in [here](https://github.com/basho-bin/bigsets "Bigsets -
basho-bin"). I won't cover the riak-core-ness of the design, I'll assume
you know about rings, replicas and vnodes.

What follows is how the prototype works today, and what I imagine
would be the next steps, but I've been wrong before.

In a (misleading) sentence bigsets is an append only log of deltas
that trades space for time, it favours writes over reads.

## 6.0 Concepts

Some things that are going to be mentioned below and I couldn't think
of a better way to include them. Then I noticed I already covered this
much lower down in 6.9. heh

### 6.0.1 Action-At-A-Distance

In all the distributed systems literature in general, and CRDT lit in
particular, the system model is a set of processes, each with a unique
ID, acting serially on some single CRDT in memory. That does not sound
a lot like Riak. Riak is a database. It stores _many_ CRDTs, durably
on disk, and the actors in Riak (the Vnodes) act as proxies for
potentially many, many clients. Sam Lenary, super-intern, described
this as "action-at-a-distance".

A Riak client gives some causal context and new value to Riak, and a
vnode acts for the client, incrementing the logical clock, and storing
the new value. CRDT Sets are even more removed as the client is
_never_ have a full copy of the set, and only sends operations to
Riak. These operations are performed for the client, but the
coordinating vnode, on it's local replica of the data. That's
action-at-a-distance. AAAD for short. Or should we call it A3D, that
sounds rad?

### 6.0.2 Contexts

The client is not a replica. The client is not a replica. The client
is not a replica. The semantic of sets in Riak is
"Observed-Remove". We only ever remove element ``X`` from the set if
client A has _seen_ element `X`, and we only remove the `X`s seen by
the client.

If the client reads a set from vnode A and asks vnode B to remove `X`,
vnode B should only remove the `X` seen by the client. If B has added
an `X` in between the clients read and subsequent update, it would be
_BAD_ to remove that `X`. To this end, the client must provide some
context for the operation. The context is some causal information that
says "I've seen this `X`" or maybe better put "I've seen `X` at time
`T`."

### 6.0.2 Adds Are Removes

And war is peace, what is this 1984? In Riak today the implementation
of the set `riak_dt_orswot` treats the add of element `X` to a set
containing element `X` as a remove of all existing dots for `X` and
the generation of new, single dot that supersedes them. This makes
sense in the "all actors are a replica" model of the CRDT
literature. After all, if a replica adds `X` to it's copy of a set, of
course it has seen all dots for `X` so the new add is not concurrent
with the existing dots for `X`, instead it super-cedes them. With A3D
(or AAAD, I haven't decided yet) we have the same context problem as
above. If client Z reads `X` from vnode A and sends an add to vnode B
it may remove some dots for `X` it has never seen. Does it matter?
Yes! It's non-deterministic (that should be enough persuasion) but it
can lead to data loss too. Client adds `X` to some node that has many
dots for `X`, then removes `X`. The client has removed `X`s
unseen. This is why Adds Are Removes + Action-At-A-Distance lead me to
thinking _all_ operation should have a context.

### 6.0.3 Per-Element-Context

In previous design iterations the set clock was the context for any
operation. This has some advantages (which we can discuss) but
ultimately, since each element has dots, it makes sense to use those
dots as the context for any element add or remove. The clock describes
a set of events extrinsic to any one element, but a
Per-Elememt-Context is explicit. It simplifies the design, as those
who read prior versions of this document will understand. See 6.9 for
more.

### 6.1. The backend

Bigsets requires a sorted backend, it uses leveldb, maybe other
backends are also suitable.

Logically at least, the biggest change is the backend storage of the
set. In riak one key maps to one value, in bigsets, we split a set
over multiple keys. See [Key Scheme](#key-scheme) for more details
about the logical on disk format. We trust the sorted property of
leveldb to ensure that each set is grouped together contiguously, and
the logical clock is the first key in each set.

![Sets in Bigsets: Decomposed](bigset-backend.png "Sets in Bigsets: Decomposed")

### 6.2. Hashing

In Riak a Bucket and Key pair are hashed to decide the preflist and
nodes that will store the data, in bigsets only the Set name is
hashed. This means that all the elements of a particular Set 'S' share
the same preference list, and are stored therefore in the same
locations. Does this mean we can model buckets as sets, and
riak_objects as elements and get something like the global logical
clocks work? Maybe.

### 6.3. <a id="key-scheme"></a>Key Scheme

The bulk of how bigset works is down to the way keys are named and
stored. Based on the observation that we can't store more than 1MB max
in a riak object, and that reading, de-serialising, inserting,
re-serialising, and writing is wasteful if all we want to do is add an
integer to a set of 1million integers, the key scheme attempts to read
as little information as possible before insert/remove.

#### 6.3.1. Clock Keys

The clock key(s) are the first keys in the Set. There are multiple
clock keys: one per actor. Each actor only reads and writes it own
clock key. An actor stores another actor's clock keys to simplify
interaction with hand off and AAE. The clock Key is a binary that is
made up of the set name, the special key designation character `c`
(for clock) and the actor name.

    <<SetNameLength:32/little-unsigned-integer,
      SetName:SetNameLength/binary,
      $c:1/binary,
      ActorName/binary>>

#### 6.3.2 Set Tombstone

In leveldb deletes are writes. Engel's idea was that rather than
issuing a delete for a key we could write our own tombstone for that
key and have level compact it out. With the way we do Hand Off (see
[Hand Off](#handoff) below) a general tombstone for dots in the whole
set was needed. This `set-tombstone` has many uses. It contains some
compact causal information in a `bigset_clock` that describes all the
events a vnode may have on disk, that it should remove. It has a part
to play in [compaction](#compaction), [writes](#writes), and also in
[reads](#read). It maybe a way to do efficient whole set deletes. It
is very much like the clock key. Each actor has their own, and only
uses their own, and only reads and writes their own. The key is a
binary that is made up of the set name, the special key designation
character `d` (for `c` < `d` < `e`) and the actor name.

    <<SetNameLength:32/little-unsigned-integer,
      SetName:SetNameLength/binary,
      $d:1/binary,
      ActorName/binary>>

#### 6.3.2. Element Keys

An element or member of the set is a client provided opaque
binary. When an element is added to the set a key is written. When an
element is removed from the set some dots are added to the
`set-tombstone`. This means there maybe more keys in on disk than
elements in the set. This also means that there are temporarily keys
to ignore in the set, we'll look at this more later in [reads](#read).

An element key is a binary made up of the Set name, a special key
designation character `e` that means `element key`, the element, the
actor that coordinated the insert/remove, and the actor's event
counter for the insert event which together make a _dot_ for the
insert.

    <<SetNameLength:32/little-unsigned-integer,
      SetName:SetNameLength/binary,
      $e:1/binary,
      ElementLen:32/little-unsigned-integer,
      Element:ElementLen/binary,
      ActorLen:32/little-unsigned-integer,
      Actor:ActorLen/binary,
      Counter:64/little-unsigned-integer>>

The comparator sorts so that keys for the same element are together
and sorted by actor, then event.

#### 6.3.3. End Key

The end key is a key that sorts highest of all. It is used for
streaming-fold's end key, and to signify the end of the set. It is made
up of the Set name and the special key designation character `z`.

    <<SetNameLen:32/little-unsigned-integer,
      SetName:SetNameLen/binary,
      $z>>

#### 6.3.4. Comparator

The comparator sorts the keys so that clock keys come first, then the
set tombstone keys, then the element keys. Finally the end-key sorts
last. The code is in C++ (my first ever!) and can be found in the
leveldb repo. It should not be there.

### 6.4. Payload

We've seen the keys, what are their values?

#### 6.4.1. Clock Value

The clock is coded in the `bigset_clock` module. It's a Version
Vector, and a set of non-contiguous dots. Any actor `A` will always
have only contiguous events for it's own clock. There's a section on
the clock [below](#bigset-clock).

#### 6.4.2 Set-Tombstone value

Another `term_to_binary` `bigset_clock`. However, eventually (in the
"eventual consistency" sense of the word) this value is empty. After a
compaction "consumes" this tombstone it is discarded (unless a new
one, from a hand off, is created in the mean time. You see
"eventually".)

#### 6.4.2. Element Value

There is no value, just the empty binary `<<>>`.

### 6.5. Write operations - Insert and Remove <a id="writes"></a>

As with Riak sets, the client sends an operation to the server, saying
`"add X to set Y (at time T)"`. We don't have a client API yet. For
now we use an internal client interface, much like Riak's local
client. The module is `bigset_client` and you can use that and the
helper module `bigset` for playing with bigsets when you attach to a
node.

When a client wants to add or remove elements to a set it sends a
request via the client.

#### 6.5.1. Write FSM

Just as in Riak an FSM hashes the set name, and sends the operation to
a vnode to coordinate. The coordinating vnode returns a payload to be
replicated, and the FSM sends the payload to N-1 vnodes. When one of
them replies the FSM tells the client `ok`. Hard coded in bigset is
the default `n_val` of `3` and `w` val of `2` and `dw` val of `2`. See
Riak docs for the meaning of these properties. This default exists for
parity with Riak defaults when benchmarking. In production these
parameters will need be variable and set via the client.

#### 6.5.2. Coorindating Vnode

Just as in Riak we need a coordinating vnode for adds. The vnode acts
as a proxy for the client. This keeps version vector size down and
simplifies life for client/application developers. Continuing with the
example started above, when a client issues an `insert` operation, the
coordinating vnode performs the following:

1. Read it's `bigset_clock` and `set-tombstone` for the Set (see `bigset:clock_key/2`)
2. For each element being inserted
    1. Increment the clock (which generates a dot)
    2. Create an "element key" for the element
    3. For each `dot` in the `per-element-ctx`
        1. if the dot is in the clock add it to the tombstone (ensures [compaction](#compaction) of key)
        2. else add it to the clock (ensures superseded key is never written)
    4. add `{put, Key, <<>>}` to a list of writes for leveldb
    5. add `{Key, Dot, Ctx}` to the `delta-list` for replicating
3. add `{put, ClockKey, Clock}` and `{put, TombstoneKey, TS}` to the `write list`
4. Write the `write list` to leveldb
5. Return the `delta list` to the write FSM
   (note, _not_ the updated bigset clock, just the list)

For a remove the operation is simpler. In fact, for an operation that
is _just_ a remove (we allow compound and bulk operations
(e.g. `{add_all, [a,b,c,d}` and `{update, [{add, x}, {remove, y}]}`
etc) we can skip coordination and just broadcast the
`per-element-ctx`s for removes to all N vnodes.

On remove:

1. Read the clock and tombstone
2. for each dot in the context(s)
    1. if the dot is in the clock add it to the tombstone (ensures [compaction](#compaction) of key)
    2. else add it to the clock (ensures superseded key is never written)
3. Write the updated clock and tombstone


#### 6.5.3. Replicating Vnode

When the `N-1` replica vnodes receive the `delta list` they store
unseen updates as follow:

0. return `w` to the write fsm
1. Read own `bigset_clock` and `set-tombstone` for the set
2. For each element in the [delta-list](#rep-list)
    1. If the `dot` for the element has been seen, do nothing
    2. If the `dot` is not seen
        1. add `dot` to `bigset_clock`
        2. add `{put, Key, <<>>}` to `write list`
    3. for each dot in the `Ctx`
    1. if the dot is in the clock add it to the tombstone (ensures [compaction](#compaction) of key)
    2. else add it to the clock (ensures superseded key is never written)
3. Add `{put, ClockKey, Clock}` and `{put, TSKey, TS}` to `write list`
4. Write the `write list` to leveldb
5. return `dw` to the write fsm

For removes see above. Basically tombstone seen dots, add unseen to
clock.

#### 6.5.4. The <a id="rep-list"></a> Delta List

What is the `delta-list`? It's deltas. Almost exactly the deltas of
[the paper](http://arxiv.org/abs/1410.2803). What are they exactly?

Each item in the `delta-list` is a binary encoded key (see
[Key Scheme](#key-scheme)) a binary encoded `context` (that is some
dots) and an unencoded `dot` (an `{actor, counter}` pair). The `dot`
saves the replica actor the effort of decoding the key, that's all it
is there for.

In the Delta paper, the delta consists of the new dot, and a context
made up of the dots removed by the new dot. Imagine an orswot with the
element `paul` with dots `[{a, 1}, {b, 6}, {c, 9}]`.  Adding `paul` by
actor `a` at event `3` would generate a delta that contained the new
dot `{a, 3}` but also a context of `[{a, 1}, {b, 6}, {c, 9}]`. The add
at `a` says "I've seen all these `paul`s at `a` so this new add
replaces them!" With bigset we can't do that because It doesn't matter
what vnode `a` has seen, what matters is what the client has seen,
which is why we insist a client reads `paul` from the set before
adding him: it provides a `context` that ensures this insert of `paul`
supersedes only seen `paul`s. There is more on this in
[contexts](#contexts) below.

### 6.6. <a id="read"></a>Read

Reading a bigset is maybe dumb. If you have a set that is actually "big" why do you want to read it all? You probably want instead to ask it questions:

* Do you contain 'X'?
* How many elements start with "Rus*"?
* Give me the first 1000 elements between "A" and "C"
* Is `[a, b, f, g]` a subset?

etc.

However, read is the simplest and first query we will deliver.

By default we _stream_ results to the client in order. We require the
vnodes to send data to the read fsm in order for the CRDT merge logic
(more below.) By default we return results in lumps as soon as we have
"enough" results. More details below, but this is partially governed
by the application setting `batch_size` which defaults to `1000`.

#### 6.6.1. Read FSM

As in Riak there is a `gen_fsm` for reads. In bigsets `r=2,
notfound_ok=true` by default. In production we will have to make this
a variable.

When asked to read a Set, a read fsm is started, a preflist generated,
and a read request sent to all `n=3` vnodes on the preflist.

See below for more details, but in brief the read fsm waits for `r=2`
clocks from the replicas, and then uses only those first `r`
responding replicas for the rest of the read. The 3rd replica is told
to stop folding/sending results.

As the read fsm received data it uses `bigset_read_core` to build a
response for the client. As noted above, results are sent as soon as
they are ready, rather than waiting to build the whole set in memory.

Conceptually you can think of a _local set_ per vnode, that is the
replica's local copy of the set, and a _global set_ that is the
"eventually consistent" view of the set, obtained by the merge of all
_local sets_ at a time of system quiescence.

It is the job of the read to build a local set per vnode, and then
merge `r` local sets into a set we send the client.

#### 6.6.2. Fold/Accumulate per vnode

A read is an async vnode worker task and does not block the vnode like
it does in riak. The main reason is that we expect our sets to be
big. Since a bigset read looks most like a 2i query read at the vnode
level, we use an async task. This needs looking at/bikshedding for
production as we need to ensure that correct size pool for async
workers, or do we just spawn a new process, or what should we do? For
now we use worker pools, with a default size of `100` per vnode.

When a read request hits a vnode it immediately hands over to a
worker. I'm hoping mixed workload benchmarking will show this to be
win.

##### 6.6.2.1. Vnode Worker

A bigset read is an eleveldb fold operation. It iterates over a
portion of the keyspace to build a portion of regular looking
optimised or-set (something like `riak_dt_orswot`). The vnode worker
sets up the fold by creating a new `bigset_fold_acc` buffer record,
creating a `start_key` and `end_key` for the `streaming_fold`
operation, and calling `eleveldb:fold/4` with the accumulator, fold
function, and options. When the eleveldb reports the fold complete,
the vnode worker cleans up by calling `finalise` on the buffer to send
any last messages, and is returned to the pool.

##### 6.6.2.2. Fold/Accumulate

We moved the fold logic to c++ in eleveldb, but I changed it for this
design, so that work will need doing again.

The main trick of the bigset design is to "just get it on disk" for
writes, and defer all the resolution logic until read time. That logic
gets run here. The fold/accumulate logic takes the log of writes in
leveldb and turns them into a local `riak_dt_orswot` in `batch_size`
lumps.

Logically a bigset is as per the diagram above, it has a clock,
elements, and an end key. The `start_key` for the fold operation is
the clock key for the vnode/replica that is being folded over. In an
`eleveldb:fold` each key from `start_key` to `end_key` is passed to
2-arity fold function, the signature of which is `Fun({Key, Value,
Accumulator})`. In bigsets that function is
`bigset_fold_acc:fold/2`. The accumulator is a record wrapped by the
`bigset_fold_acc` module.

If the first key given to the fold function is _not_ the vnode's clock
key, that is treated as a `not_found` by bigsets, and a `not_found`
message is sent from the vnode worker process to the read fsm process,
and the worker stops there, and is returned to the pool.

If the first key _is_ the vnode's clock, then the clock is decoded and
sent to the read fsm.

Due to the comparator and [key scheme](#key-scheme), before any
element keys are encountered, the vnode's set-tombstones key will be
read. The value, a `bigset_clock` is decoded and added to the
accumulator state.

Eleveldb will then call `fold/2` for every element key it
encounters. The key is decoded into its constituent parts (`element`,
`actor`, `counter`) and considered for inclusion in the local orswot.

##### 6.6.2.3. Two accumulators

The fold logic works by considering a single element at a time. There
may be multiple keys for any element, which is why the sort order
matters: the keys for element `X` must be folded over together so we
can decide if element `X` is in the local set.

The accumulator can be conceptually broken into two accumulators: the
_element accumulator_ and the _set accumulator_. An element only gets
added to the set accumulator if it "passes" the element accumulator
logic.

If an element key's `dot` is seen/covered by the set-tombstone clock,
then it is discarded/ignored/passed over, _not_ in the set.

##### 6.6.2.5. Set Accumulator

The set accumulator is a buffer for elements. When an element passes
the above element accumulator it is added to the local orswot. When
the local orswot has `batch_size` (for example 1000) elements, the
buffer is flushed and the 1000 element local orswot is sent to the
read fsm. There is some back pressure here. The buffer is only flushed
if it has received an `ack` from the read fsm for it's last
message. The buffer also monitors the read fsm, in the case that the
read fsm goes away, the folding work stops. Maybe we should
test/bikeshed the idea that the flush occurs after considering `1000`
or some other number of keys, since a bigset of millions of keys may
in fact be an empty local orswot.

We depend on the sorted set to allow us to break an orswot into
chunks. We can work on the first 1000 elements, or any `K` contiguous
elements, of an orswot and treat it as a whole set if we have the
clock for the set. All events concerning the elements in the 1000
element chunk are covered by the logical clock already at the read
fsm. Since elements are independent of each other, we can evaluate
them without the rest of the set. It is worth reading about the
concept of "extrinsic event sets" from section 3.1, definition of
`extrinsic` in the
[global version vectors paper](http://haslab.uminho.pt/tome/publications/concise-server-wide-causality-management-eventually-consistent-data-stores)
to understand why this works. More details in
[Read Core merge](#Read-core-merge) below.


#### 6.6.2.6 <a id="Read-core-merge"></a> Read Core Merge

Each of `r` vnode replicas is sending `batch_size` chunks of orswot to
the read fsm. When the read fsm receives a chunk of orswot it adds the
chunk to the `bigset_read_core` and decides if it has enough results
to perform a CRDT orswot merge on the results and return some portion
of the set to the client. If we know that the chunks are ordered, and
we have the `bigset_clock` for the whole replica, we can treat each
chunk as an orswot. In order to merge any pair of _local sets_ into an
orswot we only need to ensure that we consider a pair of chunks that
are the same subset. This is most like a streaming merge sort. The
algorithm is the same for any `r` number, but `r=2` is the simplest to
think about.

1. For each of `r` sets take the last/highest element
2. Find the least/lowest of all the `r` highest elements, call it `least-last`
3. Take the subset of each of `r` sets where the elements are `=< than least-last`
4. standard CRDT Orswot merge on the `r` subsets
5. return the merged result to the client

The subsets of each of `r` that are `> least-last` are retained and
added to as messages come into the read fsm, until such a time as they
can be merged using the algorithm above.

At the moment the client receives a list of dots per element as the
`per-element-ctx` but this seems wasteful. A query that returns a
single element is fine, but if thousands of elements each with
multiple dots are returned we are repeatedly sending 24-byte actor
IDs. I did a version that used very simple dictionary encoding,
sending a dictionary of `actor->integer` mappings to the client, and
then encoding each dot as a pair of integers. This is an area that
will need work to be efficient.

### 6.7 <a id="compaction"></a>Compaction

If we always only write, for inserts and removes, sets really would be
bigsets. The design is a kind of decomposed log of deltas to an
orswot, and orswot stands for Observed Remove Set _WITHOUT_
Tombstones. And we write a tombstone. What gives?

From [section 3](#section-3) on Deltas above we learned the biggest
cost with sets today is reading and de-serialising in order to
add/remove an element. This leads to the approach to always write, and
handle resolution at read time. Eventually we will have to remove
superseded writes and shrink the tombstone, or the disk could be full
for a set with only two active elements! Compaction is the
method. Unlike early tomb-stoning CRDTs as described in the
[comprehensive paper](https://hal.inria.fr/inria-00555588/en/) there
is no coordination required for garbage collection/tombstone
shrinking. Each vnode has the causal information it needs to remove
superseded writes and shrink tombstones unilaterally.

My initial hope was to implement this logic in the leveldb compaction
code.

This algorithm has been implemented in the eqc test `bigset_eqc` and
statistics are displayed after the run.

0. Every key whose `dot` is seen by the set-tombstone can be removed

That's it. When leveldb compaction considers a key, it reads the
set-tombstone and the key is dropped if the tombstone covers it.

Any dropped dot is accumulated in a set. This set of removed events
can then be subtracted from the set-tombstone on disk. I suggest that
leveldb somehow asks the vnode to perform this read-subtract-write of
the tombstone (so it is serialised.)

### 6.8 <a id="bigset-clock"></a>Bigset Clock

The bigset_clock is made up of a conventional base Version Vector, and
a "Dot Cloud" which is the set of events per-actor, that is not
contiguous with the base.

The implementation in the `bigset_clock.erl` module is naive. It
composes `riak_dt_vclock` and a dictionary of `actor->[counter()]`
mappings. We're going to need a more efficient representation in
production. Worse cases like a single gap, followed by 1000s of
contiguous events are the kind of thing we need to consider. Maybe
bitmaps+compression is enough?

### 6.9 <a id="contexts"></a>Contexts and Consistency

Current Riak KV sets are broken, requiring a context for remove, but
not ever using one for insert. Bigset can "operate" with or without
contexts and we need to bike shed what the "default" should be.

#### 6.9.1 Why Contexts?

But why do we even need a context? There's nothing about this in any
of the CRDT literature (except maybe SwiftCloud?)

In the (bad | good) old days the actors in Riak were the clients
themselves. A client fetches a value, it mutates the value, and it
sends the new value, and its ID to Riak. Riak increments the Version
Vector, and merges/stores the new value. From Riak 1.0 onward the
actor is the vnode. Basho super-intern Sam Lenary described this as
"Action-At-A-Distance." For the fetch-mutate-store operations AAAD
looks very much like client side actors (except you lose idempotence)
but for CRDT's operations-style interface it is quite a change.

#### 6.9.2 Action-At-A-Distance

CRDTs can be thought of as the abstract type they represent (counter,
set, map, boolean) and the structural type required to have the
convergence property (pncounter, orswot, ormap, enable-wins-flag etc.)
What is returned to the client on the read is the value that
represents the abstract type (an integer, a set of values, true or
false.) But the client wishes to operate on the CRDT. They wish to
increment the counter, add an element to the set, set the boolean flag
to `false`. Unlike `riak_object` they can't just perform the
operation, instead the send the operation to Riak and the vnode
performs it. That is Action-At-A-Distance.

The context of an operation is required for certain CRDTs that use
causality to manage the state required for enabling the merge
logic/semantics. For example: The Add-Wins Set, or ORSWOT as we know
it has a semantic that says if two clients operate concurrently,
client A removes `bob` from the Set, and client B adds `bob` to the
set, the result is that `bob` is in the set. The add wins. We use
causality to track this. We know which adds of `bob` A has seen, and
only remove those, leaving the unseen add (from B) alone. This is what
the context is for.

#### 6.9.3 Clients as actors

Why not just have each client be an actor and send the full state to
the client, have the client mutate the state, and send it back? Maybe
we should, it would be a lot easier, for me. But it would be very
painful, for you.

If the client is the actor it must both manage actor IDs (ensure
uniqueness and serial behaviour) and always use a read-your-own-writes
consistency level to ensure correct causal behaviour.

The problem with needing a context is that it requires a read. And
reading a bigset is dumb. If you want to add `paul` to friends, should
you read ALL THE FRIENDS! first? Should you just say "yo, add `paul`"?
Should you say "get me Paul, and if he's not there add him with the
context from the get"? Options below.

#### 6.9.4 Should we allow a "fetch context" operation?

Might be handy for blind removes where you want a PR=N read of clocks to
generate a tombstone?

#### 6.9.5 With Context

If we require a context for any operation it can be fetched with an
"is 'X' in Set" request for adds or removes.

NOTE: with action-at-a-distance this is the most "correct" option.

-   Add 'X' with Ctx -&gt; uses Ctx, only seen 'X' superseded
-   Rem 'X' with Ctx -&gt; uses Ctx, only seen 'X' removed

#### 6.9.5 No Context :: Use Coordinator

NOTE: this is non-deterministic, final outcome depends on the
state at the replica that coordinates. This is how Riak sets act today
on Add, though.

- Add 'X' -&gt; any 'X' at coordinator is superseded, may "remove"
    'X's unseen by writer. May not remove any 'X's if (for example)
    empty fallback coordinates
- Remove 'X' -&gt; any 'X' at coordinator is removed, may remove 'X'
    unseen by writer. May not remove any 'X's (if for example an empty
    fallback coordinates)

However, this involves a local read at the coordinator. I have not
implemented this.

#### 6.9.6 No Context :: Use Empty Context

NOTE: deterministic, but adding 'X' N times leads to N keys for 'X'.
Also removes are no-ops (lol) , but safest of the two "no ctx"
options.

- Add 'X' is concurrent with all other adds of 'X'
- remove 'X' has no effect since an empty ctx dominates no added dots

This is what I have implemented as default, no ctx provided by the
client means empty ctx.

#### 6.9.7 Batches

All of this seems kind of simple, pick the "per-elem-ctx" option,
until you consider operating on multiple elements. What if I want to
add 10k elements at once? Or want to remove 10k elements? Just using
the whole set clock as a context makes sense there. But it adds some
much space and complexity.

### 6.10 Anti-Entropy

If bigsets allows gaps in clocks, and stores redundant keys for the
sake of write speed, anti-entropy is important. The compaction algo
outlined above depends on information being propagated around swiftly,
so that superseded keys and tombstones can be removed. Usually when we
talk about Anti-Entropy in Riak we have Read Repair and AAE. I've
lumped Hand Off in with these two mechanisms, since, to my mind, it
sort of is an anti-entropy mechanism.

#### 6.10.1 Read Repair

No idea yet, imagine the need will be detected by the read fsm, and
the action will be requesting keys straight from the vnode(s). I
think, since the keys are `{Set, Element, Actor, Cnt}` and the orswot
elements are `{Element, [dot()]}` the later can be exploded into the
former and read repair will consist of sending batches during the
read-core-merge phase.

#### 6.10.2 AAE

As above, no idea, though I _hope_ regular riak AAE will do. It is the
removing of keys through compaction that will be hard to communicate
to the AAE trees. Needs work. See Zeeshan.

#### 6.10.3 <a id="handoff"></a> Hand Off

Handoff of bigsets is different to regular riak. In riak each key/value
handed off is a complete datum, with bigsets each key/value is part of
some larger whole.

The receiver needs to `merge` the incoming bigset with their own
replica of the set, but we would like to avoid reading the whole set
from the receiving vnode. To that end the receiver keeps some state in
memory for each vnode that is handing off to it (yes, many vnodes may
handoff to the receiver at the same time, but the vnode's serial
operation means these are interleaved, not concurrent operations.)

Since the sender folds in order over its contents and sends them, we
can assume that the receiver receives an ordered set of messages,
starting with clock for set `A`, then set-tombstone, then elements,
and finally an end key, followed by the same for set `B` etc. If
handoff encounters an error, it starts from the beginning.

Why can't the hand-off receiver just behave as it does when it
receives a replicated write? For each element key it is handed it can
consider the local clock, store the unseen keys, and ignore the seen
ones. The problem is compaction (see above.) If vnode `A` removes keys
that `B` has stored, how can that be communicated. Likewise, if vnode
`A` has removed keys that `B` hasn't stored. How can vnode `A` handoff
keys it doesn't have so that `B` will remove them, or even won't store
them in future?

##### 6.10.3.1 Hand-Off Receiver State

Handling keys that are sent from the handing off vnode to the receiver
is simple: If the local clock for that set has seen the key, ignore
it, otherwise add the dot to the clock and store the key and the new
clock. Just like replication.

Handling keys that are not sent (i.e. removed by the sender) requires
some state at the receiver, and some extra information from the
sender.

In the PoC code the sender prepends its vnode ID to each message it
sends so that the receiver can associate the handoff data item with a
particular sender. The receiver creates a little in memory state for
each vnode that is handing off to it. The state consists of the
sender's clock for the current set being sent, a tracker clock for the
current set, and the current set name. Ideally this
"knowing-who-sent-me-this" would be more efficiently baked into
riak-core handoff.

Per set received the receiver does the following: when it receives a
clock key it sets the in memory `tracker` to a fresh clock. When it
receives the sender's clock key it adds it to a local state
`sender_clock` When it receives an element key, it reads the dot for
the key into the `tracker`. When it receives an end key it generates a
filter from the tracker and stores it, and clears in memory state.

The tracker then is a clock containing all the events sent by the
handing off vnode. It can be compared to the sender's clock and a set
of events that the sender has removed can be deduced. We then take
that set of "Removed events" and find the intersection with the
receiver's clock. These are the keys that the receiver has seen but
must remove (NOTE: the receiver may have already removed them.) This
intersection of events is added to the set-tombstone.

The set-tombstone is a clock that contains the set of events that is
the intersection of events the sender has removed, and the receiver
has seen. The set-tombstone is used to ignore keys on fold
(read/handoff), and ultimately to remove keys at compaction (see
above.)

After each set is received the set-tombstone must be merged with the
one currently on disk for the receiving actor/set pair. The sender's
clock must also be merged with the receiver's clock, to ensure any
unseen-but-removed keys are not written at the receiver if they arrive
later from some source.



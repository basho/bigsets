# Bigset

## Why?

Set CRDTs stored in riak are just `riak_dt_orswot`s, binary encoded,
and stuffed in a `riak_object`. As a result they have a size limit of
about 1MB (as do all `riak_object`s.)

Another side effect of the design is that inserts/removes of an
element to a set takes O(n) time. Using the correct backing data
structure one would intuitively expect inserts to be O(1), and they
are, _if_ you have a `riak_dt_orswot` in memory. But riak is a
database that persists data to disk and riak is a
distributed/replciated database that replicates data over a network.

### Sets in Riak 2+

In riak 2+ the CRDT set is stored as a binary, inside a
`riak_object`. We took this decision so that a CRDT could be
replicated like a normal `riak_object`, AAE'd like a normal
`riak_object`, read repaired like a normal `riak_object`, and MDC'd
like a normal `riak_object`. In short, we wanted to do as little as
possible in terms of "teaching Riak" about CRDTs.


CRDTs in Riak 2+ are plumbed into riak in just a few places: an API
for the client libraries, an option that is passed through
`riak_kv_put_fsm`, a couple of lines in `riak_kv_vnode`, and a couple
of lines in `riak_object`'s `merge` logic. The bulk of the non-API
code is in `riak_kv_crdt`.

Throughout most of Riak a CRDT is just an opaque binary in the value
portion of a `riak_object`. This means a CRDT like a Set has two
version vectors! The one in `riak_object`, and the one in the Set
itself.

![Sets in Riak: Matryoshka!](sets_in_riak.png "Sets in Riak: Matryoshka")

#### Writes

When adding to a set in riak today the client sends an *Operation* to
Riak, something like:

    add 'X' to Set 'Y'

At the API boundary this is parsed, validated, and stuck into a record
(`crdt_op`) that contains fields for the module (`riak_dt_orswot`),
the op (`{add, 'X'}`) and a context (not used for adds, see below on
"Causal Consistency" as I believe this is a bug.)

A call to `riak_kv_crdt:new/3` returns an object that has an
empty/bottom value `riak_dt_orswot` for merging with whatever maybe
stored in Riak, and the operation is then applied inside the
vnode. The FSM does what it does for a regular PUT, and passes on the
`#crdt_op{}` record from the `Options` list to the coordinating vnode.

In the coordinating vnode, Riak does the following:

* Read the local value
* Treat incoming object as a sibling (it has no vclock, so it is a
  sibling by riak's vnode vclock rules)
* Deserialise the CRDT on disk
* Merge the incoming bottom with the present CRDT (if there is one)
* Apply the operation to the result
* Serialise the new value in a `riak_object`
* Write object to disk
* Return object to the FSM for replicating

This may all seem a little wasteful. In actual fact it is even worse,
as the merge causes the CRDT to be de-serialised, and re-serialised,
and then it is de-serialised again for the operation application, and
finally serialised for storage/replication. This was a bug/mistake and
can be addressed. However O(4n) is still O(n).

The Put FSM will then send the new value to N-1 replicas. At each
replica the following occurs:

* Read the local value
* If the incoming object's version vector descends the local:
    * Write the new value to disk
* If the local descends the incoming
    * discard incoming value
* If they are concurrent
    * run `riak_object:merge/2` which in turn calls `riak_kv_crdt:merge/2` which will
        * De-serialise the local value
        * deserialise the incoming value
        * run `riak_dt_orswot:merge/2` on the two values
    * finally serialise the result, and store on disk

There may be many things we can do to optimise this: have a single
format for in-memory and on disk/wire that does not require
serialisation seems the first step, but there is still the cost of
reading a potentially large object off disk simply to add a single
element. We replicate that whole object (see Deltas below if you're
screaming "Deltas! Deltas!" right now.) However, even if we optimise
the current implementation, there is still that `riak_object` size
limit of ~1MB.

![Sets in Riak: Inserts, not O(1)](dt-add.png "Sets in Riak: Inserts, not O(1)")

Above is a plot generated by running a single riak node, a single
client, in a tight loop, adding ten thousand random words from my
mac's dictionary file to a single CRDT set.

Below is a basho_bench run ading one hundred thousand random keys to
one thousand sets, with 50 concurrent clients. Run on a cluster of 4
machines in the Basho Boston colo.

![Sets in Riak: "Big Data?"](riak-100kelements-1ksets-50workers.png "Sets in Riak: Big Data?")

Although the plot below might show the trend more clearly, thanks to
the shorter run time and less extreme outliers

![Sets in Riak: Shorter run](short-run-dt-writes.png "Sets in Riak:Shorter run")


#### Reads

Reading a CRDT Set is just like reading a regular
`riak_object`. Though if the `riak_object` version vectors indicate
divegence, `riak_object:reconcile/1` is called which causes each
sibling to be de-serialised, merged, and the final result to be
serialised again. A bit of a waste as the API boundary simply takes
care of deserialising the result, and calling
`riak_dt_orswot:value/1`. There is some protocol buffers/json
encoding, too. I can't imagine a way that reading a set of size `n`
was not an O(n) operation, can you?


![Sets in Riak: "Reading ~40x100k per second"](20workes-1ksets-pareto-read-10mins-dt.png "Sets in Riak: Reading ~40x100k per second")

The above plot shows read operations after the previous write
benchmark was run. 20 workers, 10 minutes, pareto distribution over
the one thousand sets, reading the full set. I don't think that is too
bad, though it could certainly be improved.

#### Queries

There are none. If you want to ask questions of your set (size, is 'X'
a member, 100 lowest members etc) you have to read it, and query it in
your application. This seems wasteful if you just want the first 100
members, or to know if 'X' is in the Set, and gets more wasteful the
larger the Set is.


### What About Deltas?

In
[Efficient State-based CRDTs by Delta-Mutation](http://arxiv.org/abs/1410.2803)
Almeida et al describe a technique for avoiding the cost of full state
replication. An operation on a CRDT generates a Delta, that can be
understood as a fragment of a CRDT. This delta can be replicated and
merged, and expresses just the change of the original operation. The
delta merge has the same properties of a full state merge: Idempotent,
commutative, associative. This means it can be delivered over
unreliable networks (AKA networks.)

This seems ideal for at least part of the issues outlined
above. However merely replacing the `riak_dt_orswot` in riak with a
`riak_dt_delta_orswot` is not enough. In fact, we tried, and it was
worse.

#### Accidental Optimisation

Recall the steps at the replica above for Set writes:

* Read the local value
* If the incoming object's version vector descends the local:
    * Write the new value to disk

With a delta this can never happen. That means, for a replicated delta
operation the action at the replica can only be:

* run `riak_object:merge/2` which in turn calls `riak_kv_crdt:merge/2` which will
    * De-serialise the local value
    * deserialise the incoming value
    * run `riak_dt_orswot:merge/2` on the two values
* finally serialise the result, and store on disk

Even when there is no concurrency the price must be paid of
deserialisation, merge, and reserialise. The only time an incoming
delta can ever replace what is on disk at a replica is when it is the
first received update. In the best case, the savings are only in the
size of the data sent over the network. The full read and update at
the coordinator is as above, as is the read and merge at the
replica. Sadly the plots are misplaced, but our experiments with this
showed deltas to perform mostly worse than full state replication in
riak.

### Summary

In summary, the answer to "why bigsets?" is that a set per-object is
inefficient and restrictive.

## What is Bigset?

Bigset is the temporary name for a prototype/proof of concept
idea. The aim is to engineer a system that takes advantage of the
delta-CRDT work cited above. The fundamental difference is that rather
than a set per-object, instead the Set is decomposed into multiple
keys. At least one key per element, and an extra key for metadata.

## Design Overview

So far bigset is not in riak, but is a riak_core project. You can find
it in [here](https://github.com/basho-bin/bigsets "Bigsets -
basho-bin"). I won't cover the riak-core-ness of the design, I'll assume
you know about rings, replicas and vnodes.

What follows is how the prototype works today, and what I imagine
would be the next steps, but I've been wrong before.

### The backend

Bigsets requires a sorted backend, it uses leveldb, maybe other
backends are also suitable.

Logically at least, the biggest change is the backend storage of the
set. In riak one key maps to one value, in bigsets, we split a set
over multiple keys. See [Key Scheme](#key-scheme) for more details
about the logical on disk format. We trust the sorted property of
leveldb to ensure that each set is grouped together contiguously, and
the logical clock is the first key in each set.

![Sets in Bigsets: Decomposed](bigset-backend.png "Sets in Bigsets: Decomposed")

### Hashing

In Riak a Bucket and Key pair are hashed to decide the preflist and
nodes that will store the data, in bigsets only the Set name is
hashed. This means that all the elements of a particular Set 'S' share
the same preference list, and are stored thefore in the same
locations. Does this mean we can model buckets as sets, and
riak_objects as elements and get something like the global logical
clocks work? Maybe.

### <a id="key-scheme"></a>Key Scheme

The bulk of how bigset works is down to the way keys are named and
stored. Based on the observation that we can't store more than 1mb max
in a riak object, and that reading, deserialising, inserting,
reserialising, and writing is wasteful if all we want to do is add an
integer to a set of 1million integers, the key scheme attempts to read
as little information as possible before insert/remove.

#### Structure of a Bigset

A bigset has a clock. It's a logical clock made up of a Version Vector
and a Dot Cloud. I borrowed the name "Dot Cloud" from Carlos
Baquero. The Version Vector summerises all the contiguous events seen
at the replica. The Dot Cloud lists the non-contiguous events seen at
the replica. There are non-contiguous events because we use delta
replication. We want a Set to be stored in order. We want Sets to be
ordered too. To this end there is a custom comparator for leveldb that
ensures the sort order.

##### Clock Keys

The clock key(s) are the first keys in the Set. There are multiple
clock keys: one per actor. Each actor only reads and writes it own
clock key. An actor stores another actor's clock keys to simplify
interaction with hand off and AAE. The clock Key is a binary that is
made up of the set name, the special key designation character `c`
(for clock) and the actor name.

    <<SetNameLength:32/little-unsigned-integer,
      SetName:SetNameLength/binary,
      $c:1/binary,
      ActorName/binary>>

##### Element Keys

An element or member of the set is a client provided opaque
binary. When an element is added to the set or removed from the set, a
key is written. This means there maybe more keys in the set than
elements. This also means that there are temporary tombstones in the
set, we'll look at this more later.

An element key is a binary made up of the Set name, a special key
designation character `e` that means `element key`, the element, the
actor that coordinated the insert/remove, the actor's event counter
for the insert/remove event which together make a _dot_ for the
insert, and yes, we increment the clock for removes, and a special
tombstone designation character `a` or `r` that denotes if the
operation is an add or a remove.

    <<SetNameLength:32/little-unsigned-integer,
      SetName:SetNameLength/binary,
      $e:1/binary,
      ElementLen:32/little-unsigned-integer,
      Element:ElementLen/binary,
      ActorLen:32/little-unsigned-integer,
      Actor:ActorLen/binary,
      Counter:64/little-unsigned-integer,
      $a | $r:1/binary>>

The comparator sorts so that keys for the same element are together
and sorted by actor, then event, and finally adds `a` before removes
`r`. That last is a carry over from before the clock was incremented
for removes.

##### Tombstone Keys

Yet to be implemented as the prototype has no compaction, though they
are in the EQC model. As a result of compaction (see below) we will
remove some element keys, but need to retain their payload information
until a the set clock has seen all the events covered by the merged
contexts of all inserts and removes for that element. That information
will be stored in a special per-element tombstone key (that is also
temporary, see compaction below.)

A per-element tombstone key is made up of the Set name, the special
key designation charater `t`, the element, and the actor who made the
tombstone by compacting.

    <<SetNameLength:32/little-unsigned-integer,
      SetName:SetNameLength/binary,
      $c:1/binary,
      ActorName/binary>>

#### End Key

The end key is a key that sorts highest of all. It is used for
streaming-folds end key, and to signify the end of the set. It is made
up of the Set name and the special key designation character `z`.

    <<SetNameLen:32/little-unsigned-integer,
      SetName:SetNameLen/binary,
      $z>>

##### Comparator

The comparator sorts the keys so that clock keys come first, then
element keys, the last keys for any element 'X' will be the
per-element-tombstone keys. Finally the end-key sorts last.

#### Payload

We've seen the keys, what are their values?

##### Clock Value

The clock is coded in the `bigset_clock` module. It's a Version
Vector, and a set of non-contiguous dots. Any actor `A` will always
have only contiguous events for it's own clock.

##### Element Value

Each element key has a payload of a full `bigset_clock` as a context
of the operation the key expresses. Yes, that seems "kinda large", I'm
not sure what to do about it right now. It's a consequence of Riak's
"action-at-a-distance" model. It doesn't matter about the order of
operations as the vnodes see it, what matters is how the clients see
events. I'll cover all the "why" later, for now, just sigh and accept
it. Hopefully we can find an efficient way to encode the
context-clock-as-value. See also compaction.

It is possible that instead of the full context we could return a
per-element context at read time, and only store that. We should
benchmark the difference.

##### Per Element Tombstone Value

As above, a bigset clock. This is the merged clock of all seen inserts
for a given element. See compaction for how it gets removed.

### Write operations - Insert and Remove

As with Riak sets, the client sends an operation to the server, saying
`"add X to set Y"`. We don't have a client API yet, and I'm
hoping/aiming for API compatability with riak data types. For now we
use an internal client interface, much like Riak's local client. The
module is `bigset_client` and you can use that and the helper module
`bigset` for playing with bigsets when you attach to a node.

When a client wants to add or remove elements to a set it sends a
request via the client.

#### Write FSM

Just as in Riak an FSM hashes the set name, and sends the operation to
a vnode to coordinate. The coorindating vnode returns a payload to be
replicated, and the FSM sends the payload to N-1 vnodes. When one of
them replies the FSM tells the client `ok`. Hard coded in bigset is
the default `n_val` of `3` and `w` val of `2` and `dw` val of `2`. See
Riak docs for the meaning of these properties. This default exists for
parity with Riak defaults when benchmarking.

#### Coorindating Vnode

The coorindating vnode

#### Replicating Vnode

### Read

#### Fold/Accumulate per vnode

#### Read FSM/Core Merge

### Compaction


